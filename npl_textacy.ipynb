{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fuente: https://relopezbriega.github.io/blog/2017/09/23/procesamiento-del-lenguaje-natural-con-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textacy\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/5e/3b8391cf6ff39350b73f8421184cf6792002b5c2c17982b7c9fbd5ff36de/textacy-0.9.1-py3-none-any.whl (203kB)\n",
      "\u001b[K     |████████████████████████████████| 204kB 4.3MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn>=0.19.0 in /opt/conda/lib/python3.7/site-packages (from textacy) (0.21.3)\n",
      "Collecting jellyfish>=0.7.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/80/bcacc7affb47be7279d7d35225e1a932416ed051b315a7f9df20acf04cbe/jellyfish-0.7.2.tar.gz (133kB)\n",
      "\u001b[K     |████████████████████████████████| 143kB 12.7MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: joblib>=0.13.0 in /opt/conda/lib/python3.7/site-packages (from textacy) (0.14.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /opt/conda/lib/python3.7/site-packages (from textacy) (1.17.3)\n",
      "Requirement already satisfied: tqdm>=4.19.6 in /opt/conda/lib/python3.7/site-packages (from textacy) (4.37.0)\n",
      "Collecting srsly>=0.0.5\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/5a/9a3288b648a0c5c86d0a2ef972b0a8062ff1088658da8165b370564ef346/srsly-0.2.0-cp37-cp37m-manylinux1_x86_64.whl (185kB)\n",
      "\u001b[K     |████████████████████████████████| 194kB 10.3MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: cytoolz>=0.8.0 in /opt/conda/lib/python3.7/site-packages (from textacy) (0.10.1)\n",
      "Requirement already satisfied: networkx>=2.0 in /opt/conda/lib/python3.7/site-packages (from textacy) (2.4)\n",
      "Collecting spacy>=2.0.12\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/76/1f30264c433f9c3c84171fa03f4b6bb5f3303df7781d21554d25045873f4/spacy-2.2.3-cp37-cp37m-manylinux1_x86_64.whl (10.4MB)\n",
      "\u001b[K     |████████████████████████████████| 10.4MB 6.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cachetools>=2.0.1\n",
      "  Downloading https://files.pythonhosted.org/packages/2f/a6/30b0a0bef12283e83e58c1d6e7b5aabc7acfc4110df81a4471655d33e704/cachetools-3.1.1-py2.py3-none-any.whl\n",
      "Collecting pyphen>=0.9.4\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/15/82/08a3629dce8d1f3d91db843bb36d4d7db6b6269d5067259613a0d5c8a9db/Pyphen-0.9.5-py2.py3-none-any.whl (3.0MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0MB 16.0MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.17.0 in /opt/conda/lib/python3.7/site-packages (from textacy) (1.3.1)\n",
      "Requirement already satisfied: requests>=2.10.0 in /opt/conda/lib/python3.7/site-packages (from textacy) (2.22.0)\n",
      "Collecting pyemd>=0.5.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c0/c5/7fea8e7a71cd026b30ed3c40e4c5ea13a173e28f8855da17e25271e8f545/pyemd-0.5.1.tar.gz (91kB)\n",
      "\u001b[K     |████████████████████████████████| 92kB 6.7MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: toolz>=0.8.0 in /opt/conda/lib/python3.7/site-packages (from cytoolz>=0.8.0->textacy) (0.10.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from networkx>=2.0->textacy) (4.4.1)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading https://files.pythonhosted.org/packages/e1/79/6ce05ecf4d50344e29749ea7db7ddf427589228fb8fe89b29718c38c27c5/cymem-2.0.3-cp37-cp37m-manylinux1_x86_64.whl\n",
      "Collecting wasabi<1.1.0,>=0.4.0\n",
      "  Downloading https://files.pythonhosted.org/packages/3f/35/6dc35bc3b49e160a016e420eb4bdcf1c887db6fd33a463959c06a508c339/wasabi-0.4.0-py3-none-any.whl\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/5b/ae4da6230eb48df353b199f53532c8407d0e9eb6ed678d3d36fa75ac391c/preshed-3.0.2-cp37-cp37m-manylinux1_x86_64.whl (118kB)\n",
      "\u001b[K     |████████████████████████████████| 122kB 7.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting thinc<7.4.0,>=7.3.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/53/d11d2faa6921e55c37ad2cd56b0866a9e6df647fb547cfb69a50059d759c/thinc-7.3.1-cp37-cp37m-manylinux1_x86_64.whl (2.2MB)\n",
      "\u001b[K     |████████████████████████████████| 2.2MB 7.9MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting catalogue<1.1.0,>=0.0.7\n",
      "  Downloading https://files.pythonhosted.org/packages/4f/d5/46ff975f0d7d055cf95557b944fd5d29d9dfb37a4341038e070f212b24fe/catalogue-0.0.8-py2.py3-none-any.whl\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from spacy>=2.0.12->textacy) (41.6.0.post20191101)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading https://files.pythonhosted.org/packages/73/fc/10eeacb926ec1e88cd62f79d9ac106b0a3e3fe5ff1690422d88c29bd0909/murmurhash-1.0.2-cp37-cp37m-manylinux1_x86_64.whl\n",
      "Collecting plac<1.2.0,>=0.9.6\n",
      "  Downloading https://files.pythonhosted.org/packages/86/85/40b8f66c2dd8f4fd9f09d59b22720cffecf1331e788b8a0cab5bafb353d1/plac-1.1.3-py2.py3-none-any.whl\n",
      "Collecting blis<0.5.0,>=0.4.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0a/8c/f1b2aad385de78db151a6e9728026f311dee8bd480f2edc28a0175a543b6/blis-0.4.1-cp37-cp37m-manylinux1_x86_64.whl (3.7MB)\n",
      "\u001b[K     |████████████████████████████████| 3.7MB 7.4MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.10.0->textacy) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.10.0->textacy) (2019.9.11)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.10.0->textacy) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.10.0->textacy) (1.25.6)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.0.12->textacy) (0.23)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.0.12->textacy) (0.6.0)\n",
      "Requirement already satisfied: more-itertools in /opt/conda/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.0.12->textacy) (7.2.0)\n",
      "Building wheels for collected packages: jellyfish, pyemd\n",
      "  Building wheel for jellyfish (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jellyfish: filename=jellyfish-0.7.2-cp37-cp37m-linux_x86_64.whl size=87398 sha256=936e36bb4908a2478bd6ddec9ef654160f57cccc95e0ffe3c60db36f70da9b0d\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/e8/fe/99/d8fa8f2ef7b82a625b0b77a84d319b0b50693659823c4effb4\n",
      "  Building wheel for pyemd (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyemd: filename=pyemd-0.5.1-cp37-cp37m-linux_x86_64.whl size=379141 sha256=e50f6a88fff6568911d5cbee6360b1489e1f8353b1a6e0ec9d1d7774b454c60c\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/e4/ba/b0/1f4178a35c916b22fc51dc56f278125d4b8cfb0592e5f0cc24\n",
      "Successfully built jellyfish pyemd\n",
      "Installing collected packages: jellyfish, srsly, cymem, wasabi, murmurhash, preshed, plac, blis, thinc, catalogue, spacy, cachetools, pyphen, pyemd, textacy\n",
      "Successfully installed blis-0.4.1 cachetools-3.1.1 catalogue-0.0.8 cymem-2.0.3 jellyfish-0.7.2 murmurhash-1.0.2 plac-1.1.3 preshed-3.0.2 pyemd-0.5.1 pyphen-0.9.5 spacy-2.2.3 srsly-0.2.0 textacy-0.9.1 thinc-7.3.1 wasabi-0.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install textacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando las librerías que vamos a utilizar\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import textacy\n",
    "from textacy.datasets import Wikipedia\n",
    "from collections import Counter, defaultdict\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "\n",
    "# graficos incrustados\n",
    "%matplotlib inline\n",
    "\n",
    "# función auxiliar\n",
    "def leer_texto(texto):\n",
    "    \"\"\"Función auxiliar para leer un archivo de texto\"\"\"\n",
    "    with open(texto, 'r') as text:\n",
    "        return text.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6.18G/6.18G [1:59:33<00:00, 862kB/s]   \n"
     ]
    }
   ],
   "source": [
    "# Descargando copus de wikipedia\n",
    "wp = Wikipedia(data_dir='/home/jovyan/work/data', lang='es')\n",
    "wp.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'wikipedia',\n",
       " 'site_url': 'https://en.wikipedia.org/wiki/Main_Page',\n",
       " 'description': 'All pages for a given language- and version-specific Wikipedia site snapshot.'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chequeando la información descargada\n",
    "wp.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El Hamelin Piscina Reserva de Naturaleza Marina es una reserva de naturaleza marina protegida localizado en el Patrimonio Mundial de la UNESCO –Bahía Shark listado en la región de Gascoyne en Australia Occidental. Las 127,000-hectárea (310,000-acres) de la reserva de naturaleza presume los ejemplos más diversos y abundantes de stromatolitos, o 'fósiles vivientes', en el mu \n",
      "\n",
      "The Future Sugar es el segundo álbum de estudio del grupo de rock mexicano Rey Pila, lanzado el 25 de septiembre de 2015 bajo el sello Cult Records.[8]​ El disco fue grabado en los estudios DFA de la ciudad de Nueva York y cuenta con la producción de Chris Coady y Julian Casablancas.\n",
      "\n",
      "Todas las canciones por Diego Solórzano, Andrés Velasco y Rodrigo Blanco \"Fire Away\" – 3: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for text in wp.texts(min_len=1000, limit=2):\n",
    "    print(text[:375], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 66.7M/66.7M [00:27<00:00, 2.45MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "es\n",
      "en\n",
      "fr\n",
      "da\n",
      "it\n",
      "pt\n"
     ]
    }
   ],
   "source": [
    "# Detectando el idioma con taxtacy\n",
    "saludos = [\"Buenos días\", \"Hello\", \"Bonjour\", \"Guten Tag\", \"Buon giorno\", \"Bom dia\"]\n",
    "for saludo in saludos:\n",
    "    print(textacy.lang_utils.identify_lang(saludo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting es_core_news_sm==2.2.5\n",
      "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-2.2.5/es_core_news_sm-2.2.5.tar.gz (16.2MB)\n",
      "\u001b[K     |████████████████████████████████| 16.2MB 9.4MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /opt/conda/lib/python3.7/site-packages (from es_core_news_sm==2.2.5) (2.2.3)\n",
      "Requirement already satisfied: thinc<7.4.0,>=7.3.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (7.3.1)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (0.4.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.17.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (2.0.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (2.22.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (41.6.0.post20191101)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (3.0.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (0.2.0)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (0.0.8)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.1.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (0.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /opt/conda/lib/python3.7/site-packages (from thinc<7.4.0,>=7.3.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (4.37.0)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (2019.9.11)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (1.25.6)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->es_core_news_sm==2.2.5) (0.23)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->es_core_news_sm==2.2.5) (0.6.0)\n",
      "Requirement already satisfied: more-itertools in /opt/conda/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->es_core_news_sm==2.2.5) (7.2.0)\n",
      "Building wheels for collected packages: es-core-news-sm\n",
      "  Building wheel for es-core-news-sm (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for es-core-news-sm: filename=es_core_news_sm-2.2.5-cp37-none-any.whl size=16172936 sha256=a1fc5bf2a98d1c08e4a35c1d698832d0ca9125a311f8f8dc2717e750cacfb4cf\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-qqrkh55h/wheels/05/4f/66/9d0c806f86de08e8645d67996798c49e1512f9c3a250d74242\n",
      "Successfully built es-core-news-sm\n",
      "Installing collected packages: es-core-news-sm\n",
      "Successfully installed es-core-news-sm-2.2.5\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('es_core_news_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/opt/conda/lib/python3.7/site-packages/es_core_news_sm -->\n",
      "/opt/conda/lib/python3.7/site-packages/spacy/data/es\n",
      "You can now load the model via spacy.load('es')\n"
     ]
    }
   ],
   "source": [
    "# Cargando el modelo en español de spacy\n",
    "!python -m spacy download es\n",
    "nlp = textacy.load_spacy_lang('es')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = leer_texto('ORWELL_1984.txt')\n",
    "texto_procesado = nlp(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6435\n"
     ]
    }
   ],
   "source": [
    "sentencias = [s for s in texto_procesado.sents]\n",
    "print(len(sentencias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Winston Smith, con la barbilla clavada en el pecho en su esfuerzo por burlar el molestísimo viento, se deslizó rápidamente por entre las puertas de cristal de las Casas de la Victoria, aunque no con la suficiente rapidez para evitar que una ráfaga polvorienta se colara con él.\n",
      ", El vestíbulo olía a legumbres cocidas y a esteras viejas., Al fondo, un cartel de colores, demasiado grande para hallarse en un interior, estaba pegado a la pared., Representaba sólo un enorme rostro de más de un metro de anchura: la cara de un hombre de unos cuarenta y cinco años con un gran bigote negro y facciones hermosas y endurecidas., Winston se dirigió hacia las escaleras., Era inútil intentar subir en el ascensor., No funcionaba con frecuencia y en esta época la corriente se cortaba durante las horas de día., Esto era parte de las restricciones con que se preparaba la Semana del Odio., Winston tenía que subir a un séptimo piso., Con sus treinta y nueve años y una úlcera de varices por encima del tobillo derecho, subió lentamente, descansando varias veces.]\n"
     ]
    }
   ],
   "source": [
    "# imprimir las primeras 10 sentencias para verificar el texto\n",
    "print(sentencias[1:11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[—¿Morirá el Gran Hermano?,\n",
       " No habrá lealtad; no existirá más fidelidad que la que se debe al Partido, ni más amor que el amor al Gran Hermano.,\n",
       " Pensó en el Gran Hermano.,\n",
       " ¿Cuáles eran sus verdaderos sentimientos hacia el Gran Hermano?,\n",
       " Dime: ¿cuáles son los verdaderos sentimientos que te inspira el Gran Hermano?,\n",
       " Tienes que amar al Gran Hermano.,\n",
       " » Winston miró el retrato del Gran Hermano.,\n",
       " Volvió a mirar el retrato del Gran Hermano.,\n",
       " Amaba al Gran Hermano.\n",
       " \n",
       " \n",
       " \n",
       " \n",
       " ,\n",
       " Hubiera sido posible, por ejemplo, decir el «Gran Hermano inbueno».]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentencias con las que aparece el Gran Hermano\n",
    "[sent for sent in texto_procesado.sents if 'Gran Hermano' in sent.string][-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encontrar_personajes(doc):\n",
    "    \"\"\"\n",
    "    NO FUNCIONA\n",
    "    Devuelve una lista de los personajes de un `doc` con su cantidad de\n",
    "    ocurrencias\n",
    "    \n",
    "    :param doc: NLP documento parseado por Spacy\n",
    "    :return: Lista de Tuplas con la forma\n",
    "        [('winston', 686), (\"o'brien\", 135), ('julia', 85),]\n",
    "    \"\"\"\n",
    "    \n",
    "    personajes = Counter()\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == 'PER':\n",
    "            personajes[ent.lemma_] += 1\n",
    "            \n",
    "    return personajes.most_common()\n",
    "\n",
    "\n",
    "def encontrar_personajes_sin_counter(doc):\n",
    "    \"\"\"\n",
    "    Devuelve una lista de los personajes de un `doc` con su cantidad de\n",
    "    ocurrencias\n",
    "    \n",
    "    :param doc: NLP documento parseado por Spacy\n",
    "    :return: Lista de Tuplas con la forma\n",
    "        [('winston', 686), (\"o'brien\", 135), ('julia', 85),]\n",
    "    \"\"\"\n",
    "    personajes = dict()\n",
    "    for entity in texto_procesado.ents:\n",
    "        if entity.label_ == 'PER':\n",
    "            personajes[entity.lemma_] = personajes.get(entity.lemma_, 0) + 1\n",
    "\n",
    "    pers_list = [(k, v) for k, v in personajes.items()]         \n",
    "    pers_list = sorted(pers_list, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return pers_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Winston', 712), ('Julia', 135), (\"O'Brien\", 101), ('Había', 68), ('Parsons', 33), ('Goldstein', 28), ('Qué', 26), ('Charrington', 20), ('Cómo', 17), ('Aquí', 14), ('Oceanía', 13), ('Sabes', 13), ('Katharine', 13), ('Ampleforth', 12), ('Le', 11), ('Rutherford', 11), ('Aquello', 10), ('De', 10), ('Empezó', 10), ('Has', 10)]\n"
     ]
    }
   ],
   "source": [
    "# Extrayendo los personajes principales del texto y contando cuantas veces son nombrados.\n",
    "#print(encontrar_personajes(texto_procesado))\n",
    "print(encontrar_personajes_sin_counter(texto_procesado)[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def personaje_verbo(doc, verbo):\n",
    "    \"\"\"\n",
    "    Encontrar los personajes que utilizan determinado `verbo` en `doc`\n",
    "    \n",
    "    :param doc: NLP documento parseado por Spacy\n",
    "    :param verbo: un objeto String \n",
    "    :return: lista de personajes que utilizan `verbo`\n",
    "    \"\"\"\n",
    "    verbos = list()\n",
    "    for ent in texto_procesado.ents:\n",
    "        if ent.label_ == 'PER' and ent.root.head.lemma_ == 'decir':\n",
    "            verbos.append(ent.text)\n",
    "    contar_verbo = Counter(verbos)\n",
    "    \n",
    "    return contar_verbo.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Winston', 54),\n",
       " ('Julia', 22),\n",
       " (\"O'Brien\", 20),\n",
       " ('Parsons', 4),\n",
       " ('Había', 2),\n",
       " ('Julia—.', 2),\n",
       " ('Goldstein', 1),\n",
       " ('Syme', 1),\n",
       " ('Oye', 1),\n",
       " ('fin—. Les', 1)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encontrar personajes que utilizan determinado verbo\n",
    "personaje_verbo(texto_procesado, \"dijo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LOC', 'MISC', 'ORG', 'PER'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trabajando con las entidades del texto\n",
    "# Una entidad nombrada es cualquier objeto del mundo real como una persona,\n",
    "# ubicación, organización o producto con un nombre propio.\n",
    "\n",
    "# tipos de entidades del texto\n",
    "set(ent.label_ for ent in texto_procesado.ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Partido,\n",
       " Partido,\n",
       " «,\n",
       " «,\n",
       " «,\n",
       " policía,\n",
       " Partido,\n",
       " Partido Interior,\n",
       " Partido Interior,\n",
       " Partido]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entidades nombradas de tipo ORG\n",
    "[ent for ent in texto_procesado.ents if ent.label_ == 'ORG'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ADJ',\n",
       " 'ADP',\n",
       " 'ADV',\n",
       " 'AUX',\n",
       " 'CONJ',\n",
       " 'DET',\n",
       " 'INTJ',\n",
       " 'NOUN',\n",
       " 'NUM',\n",
       " 'PART',\n",
       " 'PRON',\n",
       " 'PROPN',\n",
       " 'PUNCT',\n",
       " 'SCONJ',\n",
       " 'SPACE',\n",
       " 'VERB'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Partes de la oración (POS)\n",
    "# En las partes de la oración se etiquetan las palabras de acuerdo a lo \n",
    "# que significan segun su contexto. Algunas de estas etiquetas pueden\n",
    "# ser: Adjetivos, verbos, adverbios, conjunciones, pronombres, sustantivos.\n",
    "\n",
    "# Etiquetas del texto\n",
    "set(token.pos_ for token in texto_procesado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['luminoso',\n",
       " 'frío',\n",
       " 'clavada',\n",
       " 'molestísimo',\n",
       " 'suficiente',\n",
       " 'polvorienta',\n",
       " 'cocidas',\n",
       " 'viejas',\n",
       " 'grande',\n",
       " 'pegado']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Etiquetas de tipo ADJ\n",
    "[token.orth_ for token in texto_procesado if token.pos_ == 'ADJ'][1:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CAPITULO',\n",
       " 'Winston',\n",
       " 'Smith',\n",
       " 'Casas',\n",
       " 'Victoria',\n",
       " 'hallarse',\n",
       " 'Winston',\n",
       " 'Semana',\n",
       " 'Odio',\n",
       " 'Winston']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Etiquetas de tipo PROPN\n",
    "[token.orth_ for token in texto_procesado if token.pos_ == 'PROPN'][1:11]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
